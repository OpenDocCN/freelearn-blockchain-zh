["```\nif(input > 10) return output = true;\n```", "```\ny = weight * x + bias\n```", "```\nSales = weight * visitors + bias\n```", "```\nSales = 0.43 * visitors + 0.9\n```", "```\n# Python implementation\ndef prediction(x, weight, bias):\n    return weight * x + bias\n```", "```\n// Solidity implementation\nfunction prediction(uint256 _x, uint256 _weight, uint256 _bias) public pure returns(uint256) {\n return _weight * _x + _bias;\n}\n```", "```\nerror = result - prediction\n```", "```\nerror = sum((result - prediction)2) / numberOfDataPoints\n```", "```\nerror = sum((result - prediction(x, weight, bias))2) / numberOfDataPoints\n```", "```\n// Our prediction function definition for you to remember how it looked like\ny = weight * x + bias\n\n// Our prediction function with random weight and bias\nprediction = 0.1 * x + 0.4\n```", "```\nprediction = 0.1 * 3520 + 0.4 = 352.4 crimes per year\n```", "```\n// Our cost function definition for you to remember how it looked like\nerror = sum((result - prediction)2) / numberOfDataPoints\n\n// Our cost function for the initial dataset\nerror = sum((20 - 352)2) / 1\n```", "```\nerror = (20 - 352)2 / 1 = 110224 \n```", "```\n# The cost function implemented in python\ndef cost(results, weight, bias, xs):\n    error = 0.0\n    numberOfDataPoints = len(xs)\n    for i in range(numberOfDataPoints):\n        error += (results[i] - (weight * xs[i] + bias)) ** 2\n    return error / numberOfDataPoints\n```", "```\n// The cost function implemented in solidity\nfunction cost(int256[] memory _results, int256 _weight, int256 _bias, int256[] memory _xs) public pure returns(int256) {\n    require(_results.length == _xs.length, 'There must be the same number of _results than _xs values');\n    int256 error = 0; // Notice the int instead of uint since we want negative values too\n    uint256 numberOfDataPoints = _xs.length;\n    for(uint256 i = 0; i < numberOfDataPoints; i++) {\n        error += (_results[i] - (_weight * _xs[i] + _bias)) * (_results[i] - (_weight * _xs[i] + _bias));\n    }\n    return error / int256(numberOfDataPoints);\n}\n```", "```\nweightDerivative = sum(-2x * (result - (x * weight + bias))) / numberOfDataPoints\n\nbiasDerivative = sum(-2 * (result - (x * weight + bias))) / numberOfDataPoints\n```", "```\n# Python implementation, returns the optimized weight and bias for that step\ndef optimizeWeightBias(results, weight, bias, xs, learningRate):\n    weightDerivative = 0\n    biasDerivative = 0\n    numberOfDataPoints = len(results)\n    for i in range(numberOfDataPoints):\n        weightDerivative += (-2 * xs[i] * (results[i] - (xs[i] * weight + bias)) / numberOfDataPoints)\n        biasDerivative += (-2 * (results[i] - (xs[i] * weight + bias)) / numberOfDataPoints)\n\n    weight -= weightDerivative * learningRate\n    bias -= biasDerivative * learningRate\n    return weight, bias\n```", "```\n// Solidity implementation\nfunction optimize(int256[] memory _results, int256 _weight, int256 _bias, int256[] memory _xs, int256 _learningRate) public pure returns(int256, int256) {\n    require(_results.length == _xs.length, 'There must be the same number of _results than _xs values');\n    int256 weightDerivative = 0;\n    int256 biasDerivative = 0;\n    uint256 numberOfDataPoints = _xs.length;\n    for(uint256 i = 0; i < numberOfDataPoints; i++) {\n        weightDerivative += (-2 * _xs[i] * (_results[i] - (_xs[i] * _weight + _bias)) / int256(numberOfDataPoints));\n        biasDerivative += (-2 * (_results[i] - (_xs[i] * _weight + _bias)) / int256(numberOfDataPoints));\n    }\n    _weight = weightDerivative * _learningRate;\n    _bias = biasDerivative * _learningRate;\n    return (_weight, _bias);\n}\n```", "```\n# Python implementation\ndef train(results, weight, bias, xs, learningRate, iterations):\n error = 0 for i in range(iterations):\n    weight, bias = optimizeWeightBias(results, weight, bias, xs, learningRate)\n    error = cost(results, weight, bias, xs)\n    print(\"Iteration: {}, weight: {:.4f}, bias: {:.4f}, error: {:.2}\".format(i, weight, bias, error))\n return weight, bias\n```", "```\n// Solidity implementation\nfunction train(int256[] memory _results, int256 _weight, int256 _bias, int256[] memory _xs, int256 _learningRate, uint256 _iterations) public pure returns(int256, int256) {\n    require(_results.length == _xs.length, 'There must be the same number of _results than _xs values');\n    int256 error = 0;\n    for(uint256 i = 0; i < _iterations; i++) {\n        (_weight, _bias) = optimize(_results, _weight, _bias, _xs, _learningRate);\n        error = cost(_results, _weight, _bias, _xs);\n    }\n    return (_weight, _bias);\n}\n```", "```\nfrom random import uniform\n\nclass LinearRegression:\n    xs = [3520, 192, 91, 9271]\n    results = [20, 3, 0, 88]\n\n    def __init__(self):\n        initialWeight = uniform(0, 1)\n        initialBias = uniform(0, 1)\n        learningRate = 0.00000004\n        iterations = 2000\n        print('Initial weight {}, Initial bias {}, Learning rate {}, Iterations {}'.format(initialWeight, initialBias, learningRate, iterations))\n        finalWeight, finalBias = self.train(self.results, initialWeight, initialBias, self.xs, learningRate, iterations)\n        finalError = self.cost(self.results, finalWeight, finalBias, self.xs)\n        print('Final weight {:.4f}, Final bias {:.4f}, Final error {:.4f}, Prediction {:.4f} out of {}, Prediction Two {:.4f} out of {}'.format(finalWeight, finalBias, finalError, self.prediction(self.xs[1], finalWeight, finalBias), self.results[1], self.prediction(self.xs[3], finalWeight, finalBias), self.results[3]))\n```", "```\n    # Python implementation\n    def prediction(self, x, weight, bias):\n        return weight * x + bias\n\n    # The cost function implemented in python\n    def cost(self, results, weight, bias, xs):\n        error = 0.0\n        numberOfDataPoints = len(xs)\n        for i in range(numberOfDataPoints):\n            error += (results[i] - (weight * xs[i] + bias)) ** 2\n        return error / numberOfDataPoints\n```", "```\n    # Python implementation, returns the optimized weight and bias for that step\n    def optimizeWeightBias(self, results, weight, bias, xs, learningRate):\n        weightDerivative = 0\n        biasDerivative = 0\n        numberOfDataPoints = len(results)\n        for i in range(numberOfDataPoints):\n            weightDerivative += -2 * xs[i] * (results[i] - (xs[i] * weight + bias))\n            biasDerivative += -2 * (results[i] - (xs[i] * weight + bias))\n\n        weight -= (weightDerivative / numberOfDataPoints) * learningRate\n        bias -= (biasDerivative / numberOfDataPoints) * learningRate\n\n        return weight, bias\n```", "```\n    # Python implementation\n    def train(self, results, weight, bias, xs, learningRate, iterations):\n        error = 0\n        for i in range(iterations):\n            weight, bias = self.optimizeWeightBias(results, weight, bias, xs, learningRate)\n            error = self.cost(results, weight, bias, xs)\n            print(\"Iteration: {}, weight: {:.4f}, bias: {:.4f}, error: {:.2f}\".format(i, weight, bias, error))\n        return weight, bias\n\n# Initialize the class\nLinearRegression()\n```", "```\npython linearRegression.py\n```", "```\npragma solidity 0.5.5;\n\ncontract MachineLearningMarketplace {}\n```", "```\npragma solidity 0.5.5;\n\ncontract MachineLearningMarketplace {\n    event AddedJob(uint256 indexed id, uint256 indexed timestamp);\n    event AddedResult(uint256 indexed id, uint256 indexed timestamp, address indexed sender);\n    event SelectedWinner(uint256 indexed id, uint256 indexed timestamp, address indexed winner, uint256 trainedIdSelected);\n\n    struct Model {\n        uint256 id;\n        string datasetUrl;\n        uint256 weight;\n        uint256 bias;\n        uint256 payment;\n        uint256 timestamp;\n        address payable owner;\n        bool isOpen;\n    }\n    mapping(uint256 => Model) public models;\n    mapping(uint256 => Model[]) public trainedModels;\n    uint256 public latestId;\n}\n```", "```\n/// @notice To upload a model in order to train it\n/// @param _dataSetUrl The url with the json containing the array of data\nfunction uploadJob(string memory _dataSetUrl) public payable {\n    require(msg.value > 0, 'You must send some ether to get your model trained');\n    Model memory m = Model(latestId, _dataSetUrl, 0, 0, msg.value, now, msg.sender, true);\n    models[latestId] = m;\n    emit AddedJob(latestId, now);\n    latestId += 1;\n}\n```", "```\n/// @notice To upload the result of a trained model\n/// @param _id The id of the trained model\n/// @param _weight The final trained weight, it must be with 10 decimals meaning that 1 weight is 1e10 so that you can do smaller fractions such as 0.01 which would be 1e8 or 100000000\n/// @param _bias The final trained bias, it must be with 10 decimals as the weight\nfunction uploadResult(uint256 _id, uint256 _weight, uint256 _bias) public {\n    Model memory m = Model(_id, models[_id].datasetUrl, _weight, _bias, models[_id].payment, now, msg.sender, true);\n    trainedModels[_id].push(m);\n    emit AddedResult(_id, now, msg.sender);\n}\n```", "```\n/// @notice To choose a winner by the sender\n/// @param _id The id of the model\n/// @param _arrayIdSelected The array index of the selected winner\nfunction chooseResult(uint256 _id, uint256 _arrayIdSelected) public {\n    Model memory m = models[_id];\n    Model[] memory t = trainedModels[_id];\n    require(m.isOpen, 'The job must be open to choose a result');\n    // If 3 days have passed the winner will be the first one, otherwise the owner is allowed to choose a winner before 3 full days\n    if(now - m.timestamp < 3 days) {\n        require(msg.sender == m.owner, 'Only the owner can select the winner');\n        t[_arrayIdSelected].owner.transfer(m.payment);\n        models[_id].isOpen = false;\n        emit SelectedWinner(_id, now, t[_arrayIdSelected].owner, t[_arrayIdSelected].id);\n    } else {\n        // If there's more than one result, send it to the first\n        if(t.length > 0) {\n            t[0].owner.transfer(m.payment);\n            emit SelectedWinner(_id, now, t[0].owner, t[0].id);\n        } else {\n            // Send it to the owner if none applied to the job\n            m.owner.transfer(m.payment);\n            emit SelectedWinner(_id, now, msg.sender, 0);\n        }\n        models[_id].isOpen = false;\n    }\n}\n```", "```\n/// @notice The cost function implemented in solidity\n/// @param _results The resulting uint256 for a particular data element\n/// @param _weight The weight of the trained model\n/// @param _bias The bias of the trained model\n/// @param _xs The independent variable for our trained model to test the prediction\n/// @return int256 Returns the total error of the model\nfunction cost(int256[] memory _results, int256 _weight, int256 _bias, int256[] memory _xs) public pure returns(int256) {\n    require(_results.length == _xs.length, 'There must be the same number of _results than _xs values');\n    int256 error = 0; // Notice the int instead of uint since we want negative values too\n    uint256 numberOfDataPoints = _xs.length;\n    for(uint256 i = 0; i < numberOfDataPoints; i++) {\n        error += (_results[i] - (_weight * _xs[i] + _bias)) * (_results[i] - (_weight * _xs[i] + _bias));\n    }\n    return error / int256(numberOfDataPoints);\n}\n```", "```\n/// @notice To get a model dataset, payment and timestamp\n/// @param id The id of the model to get the dataset, payment and timestamp\n/// @return Returns the dataset string url, payment and timestamp\nfunction getModel(uint256 id) public view returns(string memory, uint256, uint256) {\n    return (models[id].datasetUrl, models[id].payment, models[id].timestamp);\n}\n```", "```\n/// @notice To get all the proposed trained models for a particular id\n/// @param _id The id of the model created by the buyer\n/// @return uint256[], uint256[], uint256[], uint256[], address[] Returns all those trained models separated in arrays containing ids, weights, biases, timestamps and owners\nfunction getAllTrainedModels(uint256 _id) public view returns(uint256[] memory, uint256[] memory, uint256[] memory, uint256[] memory, address[] memory) {\n    uint256[] memory ids;\n    uint256[] memory weights;\n    uint256[] memory biases;\n    uint256[] memory timestamps;\n    address[] memory owners;\n    for(uint256 i = 0; i < trainedModels[_id].length; i++) {\n        Model memory m = trainedModels[_id][i];\n        ids[i] = m.id;\n        weights[i] = m.weight;\n        biases[i] = m.bias;\n        timestamps[i] = m.timestamp;\n        owners[i] = m.owner;\n    }\n    return (ids, weights, biases, timestamps, owners);\n}\n```"]